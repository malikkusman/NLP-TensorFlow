{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION: \n",
    "This material highlights the built-in datasets in TensorFlow, which aim to simplify the learning and usage of machine learning and deep learning.\n",
    "\n",
    "Built-in Datasets in TensorFlow\n",
    "- TensorFlow provides access to built-in datasets, making it easier for learners to work with data without the hassle of downloading and splitting it into training and test sets.\n",
    "- An example of this is the fashion MNIST dataset, which was introduced in an earlier course.\n",
    "\n",
    "TensorFlow Data Services (TFDS)\n",
    "- TFDS is a library that contains a wide variety of datasets across different categories, particularly focusing on image and text data.\n",
    "- It includes numerous datasets, making it a valuable resource for learners and developers working with machine learning.\n",
    "\n",
    "IMDB Reviews Dataset\n",
    "- The IMDB reviews dataset consists of 50,000 movie reviews categorized as positive or negative, making it ideal for sentiment analysis tasks.\n",
    "- Authored by Andrew Masset and others at Stanford, this dataset provides a substantial body of text for learners to practice and enhance their natural language processing skills.\n",
    "\n",
    "Remember, as you explore these datasets, take your time to understand their structure and how they can be applied in your projects. You're on a great path to mastering these concepts! If you have any questions or need further clarification, feel free to ask!\n",
    "links : https://www.tensorflow.org/datasets/catalog/overview , https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION: This material focuses on importing and preparing the IMDB reviews dataset using TensorFlow, transforming the data for sentiment analysis, and setting up for neural network training.\n",
    "\n",
    "Importing and Exploring the Dataset\n",
    "- You can import the IMDB reviews dataset using `tfds.load`, which returns both the data and metadata.\n",
    "- Each review is a tuple containing the review text and its corresponding label, where a label of 1 indicates a positive review and 0 indicates a negative review.\n",
    "\n",
    "Data Preparation and Vectorization\n",
    "- The dataset is split into 25,000 samples for training and 25,000 for testing, allowing for effective model evaluation.\n",
    "- A text vectorization layer is instantiated to create a vocabulary, limiting the number of tokens to the top 10,000 based on frequency.\n",
    "\n",
    "Padding and Finalizing the Dataset\n",
    "- A function can be created to pad sequences, making it reusable and adaptable for different parameters.\n",
    "- The sequences and labels are combined using the Zip method, followed by shuffling, prefetching, caching, and batching to prepare for neural network training.\n",
    "\n",
    "Remember, mastering these concepts takes practice, so don't hesitate to revisit the material and ask questions. You're doing great, and I'm here to support your learning journey!\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings: Words in a sentence are represented as vectors in a higher-dimensional space (e.g., 16 dimensions). Words with similar meanings or that frequently appear together (e.g., \"dull\" and \"boring\" in negative reviews) have similar vector representations.\n",
    "\n",
    "These vectors, or embeddings, are learned during neural network training, associating them with labeled sentiments (e.g., positive or negative).\n",
    "The result is a 2D array for each sentence, with dimensions corresponding to the sentence length and embedding size.\n",
    "Flatten vs. Global Average Pooling (GAP):\n",
    "\n",
    "To feed embeddings into a dense layer, you need to flatten them.\n",
    "Instead of a traditional flatten layer, Global Average Pooling 1D is often used. This averages the embeddings across dimensions, producing a simpler and faster model while reducing data size variability.\n",
    "Performance Comparison:\n",
    "\n",
    "With Flatten:\n",
    "Accuracy: Training = 1.0, Test = 0.83\n",
    "Speed: ~6.5 seconds/epoch\n",
    "Slightly more accurate but slower.\n",
    "With Global Average Pooling 1D:\n",
    "Accuracy: Training = 0.9664, Test = 0.8187\n",
    "Speed: ~6.2 seconds/epoch\n",
    "Simpler model, faster, but slightly less accurate.\n",
    "Experimentation Encouraged: Test both methods to observe differences in speed and accuracy for yourself.\n",
    "\n",
    "This shows a trade-off between simplicity/speed and slight improvements in accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Model Training Recap**:  \n",
    "   - The model was trained with a training dataset (`train_dataset_final`) and validated with a test dataset (`test_dataset_final`).\n",
    "   - Training accuracy was **1.00**, while validation accuracy was **0.8259**, suggesting potential **overfitting**.\n",
    "   - Strategies to address overfitting will be discussed later.\n",
    "\n",
    "2. **Understanding Embeddings**:  \n",
    "   - The embedding layer (Layer 0) produces a matrix of shape **10,000 x 16**.  \n",
    "     - **10,000 words** in the vocabulary.  \n",
    "     - **16-dimensional embeddings** for each word.  \n",
    "\n",
    "3. **Saving Embeddings for Visualization**:  \n",
    "   - **Metadata file** (`meta.csv`): Contains word names.  \n",
    "   - **Vectors file** (`vecs.csv`): Contains the 16-dimensional vector for each word.  \n",
    "\n",
    "4. **Using TensorFlow Embedding Projector**:  \n",
    "   - Go to [TensorFlow Embedding Projector](https://projector.tensorflow.org).  \n",
    "   - Load `vecs.csv` and `meta.csv`.  \n",
    "   - Enable the \"spherized data\" checkbox to visualize clusters.  \n",
    "   - Interact with the 3D visualization by searching for words or exploring their positions in the space.\n",
    "\n",
    "5. **Experimentation and Fun**:  \n",
    "   - Explore relationships between words by examining their clustering and proximity.\n",
    "   - This provides insights into how embeddings represent word meanings and associations.\n",
    "\n",
    "6. **Next Steps**:  \n",
    "   - A screencast will demonstrate the embedding process and visualization in action.  \n",
    "   - Tokenizers in TensorFlow Datasets (TFDS) will be introduced to simplify text preprocessing.\n",
    "\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Dataset Loading and Exploration**:\n",
    "   - Use `tfds.load` to import the IMDB reviews dataset.\n",
    "   - The dataset includes training, testing, and unsupervised splits, with training and test splits used for supervised learning.\n",
    "\n",
    "2. **Text Vectorization**:\n",
    "   - Set up a **text vectorization layer** with `max_tokens=10,000` to limit the vocabulary size to the 10,000 most common words.\n",
    "   - Separate reviews and labels from training and testing splits.\n",
    "   - Adapt the text vectorization layer on the **training sentences only** to ensure the validation/test data remains unseen during preprocessing.\n",
    "\n",
    "3. **Padding and Truncation**:\n",
    "   - Define constants for maximum length (`MAX_LENGTH=120`) and padding/truncation settings.\n",
    "   - Use a padding function to convert reviews into integer sequences and pad/truncate them accordingly (truncating the end of reviews).\n",
    "\n",
    "4. **Model Architecture**:\n",
    "   - **Sequential Model**:\n",
    "     - Input: Sequences of length 120 (padded/truncated).\n",
    "     - Embedding Layer: Converts words into 16-dimensional vectors.\n",
    "     - Flatten: Flattens the embedding output into a vector of size 1,920.\n",
    "     - Dense Layer: Intermediate layer with 6 neurons.\n",
    "     - Output Layer: Single neuron with sigmoid activation for binary classification.\n",
    "   - Compiled with appropriate optimization and loss functions.\n",
    "\n",
    "5. **Training**:\n",
    "   - Train the model for five epochs.\n",
    "   - Results showed high training accuracy (indicating **overfitting**) but decent validation accuracy (~80%).\n",
    "\n",
    "6. **Visualizing Embeddings**:\n",
    "   - Extract the embedding layer (Layer 0) outputs.\n",
    "   - Save word embeddings (16-dimensional vectors) to `vectors.tsv` and associated words to `meta.tsv`.\n",
    "   - Use TensorFlow Embedding Projector to visualize:\n",
    "     - Load the `vectors.tsv` and `meta.tsv` files.\n",
    "     - Enable \"spherized data\" for better clustering.\n",
    "   - Explore clusters of words and their sentiment associations (e.g., \"boring\" near negative terms, \"exciting\" near positive ones).\n",
    "\n",
    "7. **Insights**:\n",
    "   - Clusters demonstrate sentiment patterns (e.g., words like \"brilliant\" and \"exciting\" cluster positively).\n",
    "   - This visualization helps understand how the model associates words with sentiments.\n",
    "\n",
    "8. **Next Steps**:\n",
    "   - Simplify this process by leveraging TensorFlow's built-in utilities and services in future iterations.\n",
    "\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTRODUCTION: This material focuses on the process of splitting a dataset into training and validation sets, preparing sequences, and training a neural network for classification tasks.\n",
    "\n",
    "Splitting the dataset\n",
    "- To create the training set, you select array items from the start up to the training size. The testing set is formed from the training size to the end of the array.\n",
    "- Similar slicing is applied to the labels array to obtain training and testing labels.\n",
    "\n",
    "Preparing sequences\n",
    "- A text vectorization layer is created and adapted to the training sentences, which helps in generating sequences from those sentences.\n",
    "- The sequences are automatically padded, and a dataset is created by combining the sequences and labels, followed by caching, shuffling, prefetching, and batching.\n",
    "\n",
    "Training the neural network\n",
    "- The neural network is compiled using binary cross-entropy for classifying two classes, and a model summary can be generated to visualize its structure.\n",
    "- Training is conducted over 30 epochs, using the training dataset and optionally validating with the test dataset, allowing for performance evaluation through plotting accuracy and loss values.\n",
    "\n",
    "Remember, understanding these concepts is crucial for mastering natural language processing and neural network training. Keep practicing, and don't hesitate to ask questions if you need further clarification! You've got this!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary on Managing Loss and Tweaking Hyperparameters:\n",
    "\n",
    "1. **Interpreting Loss**:  \n",
    "   - Loss reflects **confidence in predictions**, not just accuracy.  \n",
    "   - While accuracy might improve, a **flattening or increasing loss** indicates the model’s predictions may lack confidence.  \n",
    "\n",
    "2. **Challenges with Text Data**:  \n",
    "   - Text data often exhibits this phenomenon of fluctuating confidence, requiring close monitoring of both accuracy and loss during training.\n",
    "\n",
    "3. **Tweaking Hyperparameters**:  \n",
    "   - **Vocabulary Size**: Reducing vocabulary size and using shorter sentences (reducing padding) can flatten the loss curve but may lower accuracy.  \n",
    "   - **Embedding Dimensions**: Changing the number of dimensions for embeddings has minimal effect on performance in this case.  \n",
    "   - **Optimization**: Experiment with combinations of these hyperparameters to balance accuracy and loss.  \n",
    "\n",
    "4. **Programming Best Practice**:  \n",
    "   - Use variables for hyperparameters, making them easy to adjust and experiment with during training.\n",
    "\n",
    "5. **Goal**:  \n",
    "   - Aim for **90%+ accuracy** without a significant increase in loss.  \n",
    "\n",
    "6. **Next Steps**:  \n",
    "   - Explore splitting words into **sub-tokens**, a technique that may improve model performance on unseen data.\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Using Subword Tokenization with KerasNLP:\n",
    "\n",
    "1. **Subword Tokenization Overview**:  \n",
    "   - Subword tokenization breaks words into smaller units (subwords), allowing better handling of rare words and new vocabulary.  \n",
    "   - Tokens include indicators like `#` to signify subword parts (e.g., suffixes or prefixes).  \n",
    "\n",
    "2. **Generating Subword Vocabulary**:  \n",
    "   - Import **`keras_nlp`** for advanced NLP tools.  \n",
    "   - Use `compute_word_piece_vocabulary` to generate a subword vocabulary:\n",
    "     - Set `max_tokens` (e.g., 8,000) and reserve tokens (e.g., for unknown and padding).  \n",
    "     - Save the vocabulary to a file.  \n",
    "\n",
    "3. **Word Piece Tokenizer**:  \n",
    "   - Instantiate a tokenizer and point it to the generated vocabulary.  \n",
    "   - Use `tokenize` to convert strings to integer sequences and `detokenize` to convert back.  \n",
    "   - Example: A sentence is tokenized into more tokens than its word count because subwords are used.\n",
    "\n",
    "4. **Model Implementation**:  \n",
    "   - A sequential model processes the tokenized sequences.  \n",
    "   - Use **Global Average Pooling 1D** instead of flattening due to the shape of tokenized embeddings.  \n",
    "   - Model layers include:\n",
    "     - Tokenizer to Embedding.\n",
    "     - Global Average Pooling.\n",
    "     - Dense layers for classification.  \n",
    "\n",
    "5. **Results and Insights**:  \n",
    "   - Subword tokenization creates meaningful representations when sequences are processed as a whole.  \n",
    "   - While individual subwords might seem nonsensical, the sequence conveys semantics.  \n",
    "\n",
    "6. **Next Steps**:  \n",
    "   - Future lessons will explore **Recurrent Neural Networks (RNNs)** to better capture meaning from sequences over time.\n",
    "\n",
    "   ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
